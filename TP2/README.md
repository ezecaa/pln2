# Documentaci√≥n: Sistema RAG Chatbot con Pinecone

## √çndice
1. [Introducci√≥n](#introducci√≥n)
2. [Arquitectura del Sistema](#arquitectura-del-sistema)
3. [Tecnolog√≠as Utilizadas](#tecnolog√≠as-utilizadas)
4. [Decisiones de Dise√±o](#decisiones-de-dise√±o)
5. [Problemas Encontrados y Soluciones](#problemas-encontrados-y-soluciones)
6. [Funcionamiento del Sistema](#funcionamiento-del-sistema)
7. [Gu√≠a de Uso](#gu√≠a-de-uso)
8. [Limitaciones y Hardware](#limitaciones-y-hardware)
9. [Conclusiones](#conclusiones)

---

## Introducci√≥n

Este proyecto implementa un sistema de **Retrieval-Augmented Generation (RAG)** para crear un chatbot capaz de responder preguntas sobre un CV. El sistema combina t√©cnicas de recuperaci√≥n de informaci√≥n con modelos de lenguaje para generar respuestas contextualizadas y precisas.

### Objetivos del Trabajo Pr√°ctico
- Cargar un documento (CV) y generar embeddings vectoriales
- Almacenar los vectores en Pinecone (base de datos vectorial)
- Implementar b√∫squeda por similitud coseno
- Crear un chatbot interactivo usando Streamlit

---

## Arquitectura del Sistema

El sistema sigue el patr√≥n RAG cl√°sico:

```
Usuario ‚Üí Pregunta ‚Üí Embedding ‚Üí B√∫squeda en Pinecone 
                                         ‚Üì
                                  Recuperaci√≥n de contexto
                                         ‚Üì
                            Prompt + Contexto + Pregunta
                                         ‚Üì
                                    Modelo LLM
                                         ‚Üì
                                    Respuesta
```

### Componentes Principales

1. **Embeddings**: Conversi√≥n de texto a vectores num√©ricos
2. **Vector Store (Pinecone)**: Almacenamiento y b√∫squeda de vectores
3. **Retriever**: Sistema de recuperaci√≥n por similitud
4. **LLM**: Modelo de lenguaje para generaci√≥n de respuestas
5. **Interfaz (Streamlit)**: UI para interacci√≥n con el usuario

---

## Tecnolog√≠as Utilizadas

### Librer√≠as Core
- **Streamlit**: Framework para la interfaz web
- **LangChain**: Orquestaci√≥n del pipeline RAG
- **Pinecone**: Base de datos vectorial en la nube
- **Sentence-Transformers**: Generaci√≥n de embeddings
- **Transformers (HuggingFace)**: Modelos de lenguaje

### Modelos

#### Embeddings
- **paraphrase-multilingual-MiniLM-L12-v2**
  - Dimensi√≥n: 384
  - Soporte multiling√ºe (espa√±ol/ingl√©s)
  - Optimizado para similitud sem√°ntica

#### LLMs Disponibles
1. **FLAN-T5 (Recomendado)**
   - Variantes: Small, Base, Large
   - Tipo: Seq2Seq (Text-to-Text)
   - Entrenado con instrucciones multiling√ºes

2. **GPT-2** (Problem√°tico - ver secci√≥n de problemas)
3. **mT5** (Problem√°tico - ver secci√≥n de problemas)

---

## Evoluci√≥n del Proyecto y Decisiones de Dise√±o

### Intentos Iniciales con HuggingFace Inference API

En la primera etapa del proyecto, se intent√≥ utilizar modelos de HuggingFace a trav√©s de su **Inference API** usando `HuggingFaceEndpoint`:

```python
# Intento inicial (NO FUNCION√ì)
from langchain_huggingface import HuggingFaceEndpoint

# Se intentaron varios modelos v√≠a API:
llm = HuggingFaceEndpoint(
    repo_id="google/flan-t5-base",
    huggingfacehub_api_token=api_token
)

# Tambi√©n se prob√≥ Mistral
llm = HuggingFaceEndpoint(
    repo_id="mistralai/Mistral-7B-Instruct-v0.2",
    huggingfacehub_api_token=api_token
)
```

#### Modelos Intentados v√≠a API (Todos Fallaron)

1. **FLAN-T5 Base** - Timeouts constantes
2. **Mistral-7B-Instruct-v0.2** - Rate limits excedidos, tiempos de espera prohibitivos
3. **GPT-2** - Respuestas lentas e inconsistentes

#### Problemas Encontrados

1. **L√≠mites de Rate (Requests por minuto)**: La API gratuita tiene restricciones severas
   - Plan gratuito: ~1000 requests/d√≠a
   - Mistral-7B: Especialmente problem√°tico por ser muy popular
2. **Timeouts frecuentes**: Los modelos tardan en "despertar" si no est√°n en memoria
   - Mistral-7B: Hasta 2-3 minutos de cold start
   - Timeouts de 30 segundos hac√≠an imposible su uso
3. **Dependencia de conexi√≥n**: No funciona offline
4. **Cuotas limitadas**: El plan gratuito se agota r√°pidamente
5. **Colas de espera**: Modelos grandes como Mistral tienen prioridad baja en plan gratuito

#### Soluci√≥n Adoptada: Modelos Locales

Se decidi√≥ cambiar a **modelos locales** usando `HuggingFacePipeline`:

```python
from transformers import AutoModelForSeq2SeqLM, pipeline
from langchain_huggingface import HuggingFacePipeline

# Cargar modelo localmente
model = AutoModelForSeq2SeqLM.from_pretrained(model_id)
pipe = pipeline("text2text-generation", model=model, tokenizer=tokenizer)
llm = HuggingFacePipeline(pipeline=pipe)
```

**¬øPor qu√© no se intent√≥ Mistral-7B local?**
- **Tama√±o prohibitivo**: ~15 GB de modelo
- **Requisitos de RAM**: 16+ GB solo para el modelo
- **Sin GPU**: Generaci√≥n extremadamente lenta (minutos por respuesta)
- **Incompatible con el objetivo**: Sistema gratuito accesible en hardware modesto

**Ventajas de modelos locales m√°s peque√±os (FLAN-T5)**:
- ‚úÖ Sin l√≠mites de requests
- ‚úÖ Funciona offline
- ‚úÖ Latencia predecible
- ‚úÖ Control total sobre par√°metros
- ‚úÖ Tama√±o manejable (1-3 GB)

**Desventajas**:
- ‚ùå Requiere 2-6 GB RAM seg√∫n modelo
- ‚ùå Primera carga lenta (descarga del modelo)
- ‚ùå Sin GPU, generaci√≥n m√°s lenta que API (pero m√°s estable)

### Eliminaci√≥n de GPT-2 y mT5

Durante el desarrollo se probaron m√∫ltiples modelos, pero **GPT-2 y mT5 fueron eliminados de la versi√≥n final** por no responder adecuadamente a las necesidades del sistema RAG.

#### Modelos Descartados

```python
# C√ìDIGO ELIMINADO - Estos modelos NO est√°n en la versi√≥n final

# GPT-2 - DESCARTADO
"GPT-2 Medium": {
    "model_id": "gpt2-medium",
    "type": "causal"
}

# mT5 - DESCARTADO  
"mT5 Base": {
    "model_id": "google/mt5-base",
    "type": "seq2seq"
}
```

#### Razones de Eliminaci√≥n

**GPT-2 fue descartado porque**:
- Generaba respuestas irrelevantes o inventadas
- No segu√≠a las instrucciones del prompt
- Tend√≠a a continuar el texto en lugar de responder
- Calidad inaceptable para un sistema de Q&A

**mT5 fue descartado porque**:
- Respuestas gen√©ricas o vac√≠as
- Requer√≠a formato de prompt muy espec√≠fico
- Sin instruction-tuning, no comprende la tarea
- Necesitar√≠a fine-tuning adicional (fuera del alcance)

#### Versi√≥n Final: Solo FLAN-T5

La versi√≥n final del c√≥digo **√∫nicamente incluye variantes de FLAN-T5**:

```python
model_options = {
    "FLAN-T5 Base (Recomendado)": {
        "model_id": "google/flan-t5-base",
        "type": "seq2seq",
        "description": "Modelo equilibrado, bueno para espa√±ol"
    },
    "FLAN-T5 Small (R√°pido)": {
        "model_id": "google/flan-t5-small",
        "type": "seq2seq",
        "description": "M√°s r√°pido pero menos preciso"
    },
    "FLAN-T5 Large (Mejor calidad)": {
        "model_id": "google/flan-t5-large",
        "type": "seq2seq",
        "description": "Mejor calidad pero m√°s lento"
    }
}
```

Estos modelos demostraron ser los **√∫nicos viables** para el sistema RAG sin necesidad de fine-tuning adicional.

---

## Decisiones de Dise√±o

### 1. Modelo de Embeddings

**Decisi√≥n**: Usar `paraphrase-multilingual-MiniLM-L12-v2`

**Justificaci√≥n**:
- Soporte nativo para espa√±ol
- Balance entre calidad y velocidad
- Dimensionalidad manejable (384)
- Optimizado para tareas de similitud sem√°ntica

### 2. Chunking Strategy

```python
chunk_size=200
chunk_overlap=50
separators=["\n\n", "\n", ".", "!", "?", " ", ""]
```

**Justificaci√≥n**:
- **200 caracteres**: Mantiene contexto coherente sin ser excesivo
- **50 de overlap**: Evita perder informaci√≥n en los l√≠mites
- **Separadores jer√°rquicos**: Respeta estructura natural del texto

### 3. Selecci√≥n de FLAN-T5 como Modelo Principal

**Ventajas sobre GPT-2/mT5**:
- Arquitectura Seq2Seq dise√±ada para seguir instrucciones
- Entrenamiento con "instruction tuning"
- Mejor comprensi√≥n de contexto estructurado
- Soporte multiling√ºe nativo

### 4. Sistema de Cach√©

```python
@st.cache_resource
def load_llm_model(...):
    ...
```

**Justificaci√≥n**:
- Los modelos son pesados (>500MB)
- Evita recargas innecesarias
- Mejora experiencia del usuario

### 5. Validaci√≥n de Pinecone

El sistema incluye:
- Verificaci√≥n de autenticaci√≥n
- Manejo de l√≠mites del plan gratuito (5 √≠ndices)
- Reconexi√≥n a √≠ndices existentes
- Verificaci√≥n de estado (vectores cargados)

---

## Problemas Encontrados y Soluciones

### üî¥ Problema 1: Modelos de HuggingFace Inference API No Cargaban

#### S√≠ntoma Inicial

Al intentar usar `HuggingFaceEndpoint` para acceder a modelos remotos:

```python
from langchain_huggingface import HuggingFaceEndpoint

llm = HuggingFaceEndpoint(
    repo_id="google/flan-t5-base",
    huggingfacehub_api_token=token
)
```

**Errores encontrados**:
- `TimeoutError`: El modelo tarda demasiado en responder
- `RateLimitError`: L√≠mite de requests excedido
- `503 Service Unavailable`: Modelo no disponible
- Respuestas inconsistentes y lentas

#### Causa Ra√≠z

**Limitaciones de la Inference API gratuita**:
1. **Cold start**: Si el modelo no est√° en memoria, tarda minutos en cargar
2. **Rate limits**: ~1000 requests/d√≠a en plan gratuito
3. **Colas de espera**: Usuarios comparten recursos
4. **Timeouts agresivos**: 30 segundos m√°ximo por request

#### Soluci√≥n: Migraci√≥n a Modelos Locales

Se cambi√≥ completamente a `HuggingFacePipeline` con modelos descargados:

```python
from transformers import AutoModelForSeq2SeqLM, pipeline
from langchain_huggingface import HuggingFacePipeline

model = AutoModelForSeq2SeqLM.from_pretrained(model_id)
tokenizer = AutoTokenizer.from_pretrained(model_id)
pipe = pipeline("text2text-generation", model=model, tokenizer=tokenizer)
llm = HuggingFacePipeline(pipeline=pipe)
```

**Resultado**: Sistema estable, predecible y sin l√≠mites de uso.

---

### üî¥ Problema 2: GPT-2 Genera Respuestas Irrelevantes (MODELO ELIMINADO)

> **‚ö†Ô∏è IMPORTANTE**: GPT-2 fue **completamente eliminado** de la versi√≥n final por generar respuestas inadecuadas.

#### ¬øPor qu√© GPT-2 fue descartado?

**1. Arquitectura Causal (Autoregresivo)**

GPT-2 est√° dise√±ado para **continuar texto**, no para responder preguntas:

```
Input: "El CV dice que trabaj√≥ en X. ¬øD√≥nde trabaj√≥?"
GPT-2 piensa: "Debo continuar esta historia..."
Output: "Trabaj√≥ en varios lugares interesantes y adem√°s..."
```

**2. Entrenamiento sin Instrucciones**

- GPT-2 fue entrenado con texto de internet sin formato de Q&A
- No comprende la estructura "Contexto ‚Üí Pregunta ‚Üí Respuesta"
- Tiende a generar texto creativo en lugar de respuestas factuales

**3. Problema del pad_token**

```python
# GPT-2 no tiene pad_token por defecto
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
```

Sin esto, el modelo no puede procesar correctamente batches.

**4. Fuga de Contexto**

GPT-2 no diferencia bien entre el contexto recuperado y la pregunta, mezclando ambos en su generaci√≥n.

#### Ejemplos de Respuestas Problem√°ticas

```python
# Pregunta: "¬øCu√°l es la experiencia laboral de Mar√≠a?"
# Respuesta GPT-2: "La experiencia laboral de Mar√≠a es muy interesante 
# y variada, incluyendo m√∫ltiples proyectos en diferentes √°reas..."
# ‚ùå Respuesta gen√©rica sin datos reales del CV

# Pregunta: "¬øD√≥nde estudi√≥?"
# Respuesta GPT-2: "Estudi√≥ en una prestigiosa universidad donde..."
# ‚ùå Inventando informaci√≥n no presente en el contexto
```

#### Decisi√≥n: Eliminaci√≥n Completa

**GPT-2 NO est√° en la versi√≥n final del c√≥digo**. Fue reemplazado exclusivamente por FLAN-T5.

---

### üî¥ Problema 3: mT5 No Funcionaba Bien (MODELO ELIMINADO)

> **‚ö†Ô∏è IMPORTANTE**: mT5 fue **completamente eliminado** de la versi√≥n final por respuestas inadecuadas.

#### ¬øPor qu√© mT5 fue descartado?

**1. Entrenamiento No Supervisado**

- mT5 base fue pre-entrenado solo con tareas de "span corruption"
- No fue entrenado espec√≠ficamente para Q&A
- Necesita fine-tuning para tareas espec√≠ficas

**2. Formato de Prompt Incorrecto**

mT5 espera prefijos espec√≠ficos:

```python
# Formato correcto para mT5
prompt = "question: ¬øD√≥nde estudi√≥? context: Mar√≠a estudi√≥ en..."

# Formato usado (incorrecto para mT5 base)
prompt = "Contexto:\n{context}\n\nPregunta:\n{question}"
```

**3. Tokenizaci√≥n Problem√°tica**

mT5 usa SentencePiece, que puede tener problemas con formato de prompts no est√°ndar.

#### Ejemplos de Respuestas Problem√°ticas

```python

# Pregunta: "Describe la experiencia laboral"
# Respuesta mT5: ""
# ‚ùå Respuesta vac√≠a
```

#### Decisi√≥n: Eliminaci√≥n Completa

**mT5 NO est√° en la versi√≥n final del c√≥digo**. Fue reemplazado exclusivamente por FLAN-T5.

---

### üü¢ Soluci√≥n Final: FLAN-T5 (√öNICO MODELO EN VERSI√ìN FINAL)

> **‚úÖ La versi√≥n final del c√≥digo SOLO incluye variantes de FLAN-T5**. GPT-2 y mT5 fueron completamente eliminados.

#### ¬øPor qu√© FLAN-T5 funciona mejor?

**1. Instruction Tuning**

FLAN-T5 fue entrenado espec√≠ficamente con instrucciones:

```
Task: Responde bas√°ndote en el contexto
Input: [contexto] Pregunta: [pregunta]
Output: [respuesta]
```

**2. Arquitectura Seq2Seq Apropiada**

- **Encoder**: Procesa contexto + pregunta juntos
- **Decoder**: Genera respuesta de forma independiente
- No intenta "continuar" el texto como GPT-2

**3. Entrenamiento Multiling√ºe con Instrucciones**

- Datasets de Q&A en m√∫ltiples idiomas
- Comprende la tarea sin necesitar fine-tuning adicional

**4. No Necesita return_full_text**

```python
if model_type == "causal":
    pipeline_kwargs["return_full_text"] = False
# FLAN-T5 (seq2seq) NO necesita esto
```

---

### üî¥ Problema 4: L√≠mites de Pinecone (Plan Gratuito)

#### S√≠ntoma
```
Error: max serverless indexes allowed
```

#### Causa
- Plan gratuito: m√°ximo 5 √≠ndices
- Cada prueba creaba un √≠ndice nuevo

#### Soluci√≥n Implementada

```python
if "max serverless indexes allowed" in str(e):
    st.error("‚õî L√≠mite de √≠ndices alcanzado")
    st.info("üí° Ve a app.pinecone.io y elimina √≠ndices")
    st.stop()
```

Adem√°s, el sistema verifica si el √≠ndice ya existe antes de crear uno nuevo.

---

### üî¥ Problema 5: √çndices Vac√≠os

#### S√≠ntoma
El sistema se conecta al √≠ndice pero no devuelve resultados.

#### Causa
- √çndice creado pero vectores no cargados
- Proceso de carga interrumpido

#### Soluci√≥n

```python
stats = index.describe_index_stats()
if stats.total_vector_count == 0:
    st.warning("‚ö†Ô∏è √çndice vac√≠o. Re-indexando...")
    vectorstore.add_documents(docs)
```

---

### üî¥ Problema 6: Autenticaci√≥n de Pinecone

#### Soluci√≥n

```python
try:
    existing_indexes = pinecone_client.list_indexes()
except Exception as e:
    if "401" in str(e) or "Unauthorized" in str(e):
        st.error("‚õî API Key inv√°lida")
        st.stop()
```

Validaci√≥n temprana evita errores cr√≠pticos m√°s adelante.

---

## Funcionamiento del Sistema

### Fase 1: Inicializaci√≥n

```python
# 1. Cargar modelo de embeddings
embeddings = CustomHuggingFaceEmbeddings(
    "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)

# 2. Conectar a Pinecone
pinecone_client = Pinecone(api_key=..., environment=...)

# 3. Cargar LLM
llm = load_llm_model(model_id, model_type, ...)
```

### Fase 2: Procesamiento de Documentos

```python
# 1. Cargar CV
loader = TextLoader("cv_mariela.txt")
documents = loader.load()

# 2. Dividir en chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,
    chunk_overlap=50
)
docs = text_splitter.split_documents(documents)

# 3. Generar embeddings y subir a Pinecone
vectorstore = PineconeVectorStore.from_documents(
    docs, embeddings, index_name=index_name
)
```

### Fase 3: Pipeline RAG

```python
# 1. Usuario hace pregunta
prompt = "¬øD√≥nde estudi√≥ Mar√≠a?"

# 2. Convertir pregunta a embedding
query_embedding = embeddings.embed_query(prompt)

# 3. Buscar en Pinecone (similitud coseno)
relevant_docs = retriever.invoke(prompt)  # Top-k documentos

# 4. Construir prompt con contexto
context = "\n\n".join([doc.page_content for doc in relevant_docs])
full_prompt = f"""Contexto:
{context}

Pregunta:
{prompt}

Respuesta:"""

# 5. Generar respuesta
response = llm.invoke(full_prompt)
```

### B√∫squeda por Similitud Coseno

Pinecone calcula autom√°ticamente:

```
similitud = cos(Œ∏) = (A ¬∑ B) / (||A|| √ó ||B||)
```

Donde:
- **A**: Embedding de la pregunta
- **B**: Embedding de cada chunk en la base de datos

Retorna los chunks con mayor similitud (m√°s cercanos al 1.0).

---

## Gu√≠a de Uso

### Requisitos Previos

1. **Crear archivo `cv_mariela.txt`** con el contenido del CV
2. **Obtener API Key de Pinecone**:
   - Registrarse en [pinecone.io](https://www.pinecone.io)
   - Crear un proyecto
   - Copiar API Key y Environment

### Instalaci√≥n

```bash
pip install streamlit pinecone-client langchain langchain-community \
    langchain-huggingface langchain-pinecone sentence-transformers \
    transformers torch
```

### Ejecuci√≥n

```bash
streamlit run app.py
```

### Configuraci√≥n Inicial

1. Ingresar **Pinecone API Key** en la barra lateral
2. Ingresar **Environment** (ej: `gcp-starter`)
3. Definir nombre del √≠ndice (ej: `my-rag-index`)
4. Seleccionar modelo LLM (FLAN-T5 Base recomendado)
5. Ajustar par√°metros de generaci√≥n si es necesario

### Uso del Chat

1. Esperar a que se carguen los modelos (primera vez ~2-5 min)
2. Hacer clic en "Probar Recuperaci√≥n" para verificar funcionamiento
3. Escribir preguntas en el chat
4. El sistema recuperar√° contexto y generar√° respuestas

### Ejemplo de Preguntas

```
- ¬øCu√°l es la experiencia laboral de Mar√≠a?
- ¬øD√≥nde estudi√≥?
- ¬øQu√© habilidades tiene?
- Describe su formaci√≥n acad√©mica
```

---

## Par√°metros de Generaci√≥n

### Temperature (0.01 - 1.0)
- **Baja (0.1)**: Respuestas m√°s deterministas y conservadoras
- **Alta (0.9)**: Respuestas m√°s creativas y variables

### Top P (0.1 - 1.0)
- **Nucleus sampling**: Considera tokens que sumen hasta este percentil
- **0.95**: Balance recomendado

### Repetition Penalty (1.0 - 2.0)
- Penaliza tokens repetidos
- **1.1**: Previene bucles sin limitar expresividad

### Max Length (64 - 512)
- Tokens m√°ximos a generar
- **256**: Adecuado para respuestas concisas

---

## Comparaci√≥n de Modelos (Probados vs Final)

### Modelos Probados Durante el Desarrollo

| Modelo | Tipo | M√©todo | Estado | Resultado | Raz√≥n de Eliminaci√≥n |
|--------|------|--------|--------|-----------|---------------------|
| **Mistral-7B-Instruct-v0.2** | Causal | API Remota | ‚ùå **DESCARTADO** | üíÄ Inviable | Timeouts, rate limits, tama√±o prohibitivo |
| **FLAN-T5 Base** | Seq2Seq | API Remota ‚Üí Local | ‚ö†Ô∏è API fall√≥ ‚Üí ‚úÖ **Local OK** | ‚≠ê‚≠ê √âxito | Migrado a local |
| **FLAN-T5 Small** | Seq2Seq | Local | ‚úÖ **En versi√≥n final** | ‚≠ê Funciona bien | R√°pido y ligero |
| **FLAN-T5 Base** | Seq2Seq | Local | ‚úÖ **En versi√≥n final** | ‚≠ê‚≠ê Recomendado | Balance √≥ptimo |
| **FLAN-T5 Large** | Seq2Seq | Local | ‚úÖ **En versi√≥n final** | ‚≠ê‚≠ê‚≠ê Mejor calidad | Alta calidad |
| **GPT-2** | Causal | Local | ‚ùå **ELIMINADO** | üíÄ No funciona | Respuestas irrelevantes/inventadas |
| **GPT-2 Medium** | Causal | Local | ‚ùå **ELIMINADO** | üíÄ No funciona | Respuestas irrelevantes/inventadas |
| **mT5 Base** | Seq2Seq | Local | ‚ùå **ELIMINADO** | üíÄ No funciona | Respuestas vac√≠as/gen√©ricas |

### C√≥digo Final: Solo FLAN-T5

```python
# VERSI√ìN FINAL - Solo estos modelos est√°n disponibles
model_options = {
    "FLAN-T5 Base (Recomendado)": {
        "model_id": "google/flan-t5-base",
        "type": "seq2seq",
        "description": "Modelo equilibrado, bueno para espa√±ol"
    },
    "FLAN-T5 Small (R√°pido)": {
        "model_id": "google/flan-t5-small",
        "type": "seq2seq",
        "description": "M√°s r√°pido pero menos preciso"
    },
    "FLAN-T5 Large (Mejor calidad)": {
        "model_id": "google/flan-t5-large",
        "type": "seq2seq",
        "description": "Mejor calidad pero m√°s lento"
    }
}

# GPT-2 y mT5 fueron ELIMINADOS completamente del c√≥digo
```

### Caracter√≠sticas Comparativas

| Caracter√≠stica | FLAN-T5 ‚úÖ | Mistral-7B ‚ùå | GPT-2 ‚ùå | mT5 ‚ùå |
|---------------|-----------|--------------|---------|--------|
| Sigue instrucciones | ‚úÖ Excelente | ‚úÖ Excelente (pero inaccesible) | ‚ùå No | ‚ùå Sin fine-tuning |
| Respuestas factuales | ‚úÖ S√≠ | ‚úÖ S√≠ (pero inaccesible) | ‚ùå Inventa informaci√≥n | ‚ùå Respuestas vac√≠as |
| Viabilidad API gratuita | ‚ö†Ô∏è Limitada | ‚ùå Timeouts constantes | ‚ö†Ô∏è Muy lenta | ‚ö†Ô∏è Limitada |
| Viabilidad local | ‚úÖ Excelente | ‚ùå Requiere 16+ GB RAM | ‚úÖ Funciona pero mal | ‚úÖ Funciona pero mal |
| Tama√±o del modelo | ‚úÖ 1-3 GB | ‚ùå ~15 GB | ‚úÖ 500 MB - 1.5 GB | ‚úÖ 1-2 GB |
| Soporte espa√±ol | ‚úÖ Bueno | ‚úÖ Excelente | ‚ö†Ô∏è Limitado | ‚úÖ Bueno (pero no funcional) |
| Arquitectura para Q&A | ‚úÖ Seq2Seq ideal | ‚úÖ Causal (buena con instrucciones) | ‚ùå Causal inadecuada | ‚ö†Ô∏è Seq2Seq sin entrenar |
| Calidad RAG | ‚úÖ Alta | ü§∑ No pudo probarse | ‚ùå Muy baja | ‚ùå Baja |
| En versi√≥n final | ‚úÖ **S√ç** | ‚ùå **NO** | ‚ùå **NO** | ‚ùå **NO** |

### ¬øPor qu√© Mistral-7B fue descartado?

**Mistral-7B-Instruct-v0.2** es objetivamente un modelo superior a FLAN-T5, pero result√≥ **completamente inviable** para este proyecto:

#### Problemas con API Remota
```python
# Intento fallido
llm = HuggingFaceEndpoint(
    repo_id="mistralai/Mistral-7B-Instruct-v0.2",
    huggingfacehub_api_token=token
)
# Resultado: Timeouts de 60+ segundos, rate limits agotados en minutos
```

#### Problemas con Ejecuci√≥n Local
- **Tama√±o**: ~15 GB (vs 1-3 GB de FLAN-T5)
- **RAM necesaria**: 16+ GB solo para el modelo
- **Sin GPU**: 3-5 minutos por respuesta (vs 5-15 segundos de FLAN-T5)
- **Objetivo del proyecto**: Sistema gratuito y accesible en hardware modesto

#### Decisi√≥n Final
Aunque Mistral-7B hubiera dado mejores respuestas, fue **descartado por ser incompatible con los objetivos del trabajo pr√°ctico** (sistema gratuito y accesible).

---

## Comparaci√≥n de Modelos

---

## Arquitectura T√©cnica Detallada

### Clase CustomHuggingFaceEmbeddings

```python
class CustomHuggingFaceEmbeddings(Embeddings):
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        # Convierte m√∫ltiples textos a vectores
        return self.model.encode(texts, convert_to_numpy=True).tolist()
    
    def embed_query(self, text: str) -> List[float]:
        # Convierte una pregunta a vector
        return self.model.encode([text], convert_to_numpy=True)[0].tolist()
```

**Prop√≥sito**: Adaptar Sentence-Transformers a la interfaz de LangChain.

### Sistema de Cach√©

- `@st.cache_resource`: Mantiene modelos en memoria entre ejecuciones
- Bot√≥n de limpieza para recargar con nuevos par√°metros
- Evita descargas repetidas (modelos se guardan en `~/.cache/huggingface`)

### Manejo de Errores

El sistema incluye validaci√≥n en m√∫ltiples capas:

1. **Configuraci√≥n**: Verifica API keys antes de iniciar
2. **Conexi√≥n**: Maneja errores de red con Pinecone
3. **Carga de modelos**: Fallback a tokenizador lento si falla r√°pido
4. **Generaci√≥n**: Try-catch en el pipeline de chat

---

## Limitaciones Conocidas

1. **Plan Gratuito de Pinecone**:
   - M√°ximo 5 √≠ndices
   - 100,000 vectores por √≠ndice
   - Latencia variable

2. **Modelos Locales**:
   - FLAN-T5 Base requiere ~3GB RAM
   - Primera carga lenta (~2-5 min)
   - Sin GPU, la generaci√≥n es lenta (5-15 seg/respuesta)

3. **Calidad de Respuestas**:
   - Depende de la calidad del chunking
   - Chunks muy peque√±os pierden contexto
   	- Chunks muy grandes reducen precisi√≥n de b√∫squeda

---

## Limitaciones y Hardware

### Requerimientos de Hardware
El sistema est√° dise√±ado para funcionar en entornos locales, pero tiene ciertos requisitos m√≠nimos:

*   **CPU**: Procesador moderno (Intel i5/i7 o Apple Silicon M1/M2/M3).
*   **RAM**: M√≠nimo **8 GB** (16 GB Recomendado). El modelo `flan-t5-base` consume aprox 1-2 GB en memoria, m√°s el overhead de Python y Streamlit.
*   **GPU**: No es estrictamente necesaria (funciona en CPU), pero mejora significativamente la velocidad de inferencia. En equipos Mac con Apple Silicon, se utiliza la aceleraci√≥n MPS (Metal Performance Shaders).

### Limitaciones del Modelo
*   **Context Window**: FLAN-T5 tiene un l√≠mite de entrada de 512 tokens. Esto restringe la cantidad de contexto que se puede pasar al modelo, lo que a veces puede limitar respuestas que requieran analizar textos muy extensos.
*   **Alucinaciones**: Aunque el RAG reduce este problema, los modelos peque√±os como `base` a√∫n pueden inventar informaci√≥n si el contexto recuperado es insuficiente.

---

## Conclusiones

4. **Multiling√ºe**:
   - FLAN-T5 funciona mejor en ingl√©s
   - Espa√±ol tiene buena calidad pero no √≥ptima

---

## Mejoras Futuras

### Corto Plazo
- [ ] Agregar historial de conversaci√≥n persistente
- [ ] Mostrar fuentes (chunks recuperados) en la respuesta
- [ ] Implementar filtros por relevancia (score threshold)
- [ ] Agregar m√©tricas de evaluaci√≥n (precisi√≥n, recall)

### Mediano Plazo
- [ ] Soporte para m√∫ltiples documentos (varios CVs)
- [ ] Fine-tuning de FLAN-T5 en dataset de CVs
- [ ] Implementar re-ranking de resultados
- [ ] Sistema de feedback para mejorar respuestas

### Largo Plazo
- [ ] Migrar a modelos m√°s grandes (FLAN-T5 XL)
- [ ] Implementar hybrid search (vectorial + keyword)
- [ ] Despliegue en cloud con GPU
- [ ] API REST para integraci√≥n externa

---

## Conclusiones

### ‚úÖ Logros

1. **Sistema RAG funcional** con stack completamente gratuito
2. **Identificaci√≥n y eliminaci√≥n de modelos inadecuados** (GPT-2, mT5)
3. **Documentaci√≥n exhaustiva de problemas** encontrados durante el desarrollo
4. **Migraci√≥n exitosa de API remota a modelos locales**
5. **Interfaz intuitiva** con Streamlit
6. **Manejo robusto de errores** en Pinecone
7. **Configuraci√≥n flexible** con solo modelos viables (FLAN-T5)

### üìö Aprendizajes Clave

1. **No todos los LLMs son apropiados para RAG**:
   - Seq2Seq > Causal para Q&A
   - Instruction-tuned > Pre-trained
   - **GPT-2 y mT5 fueron descartados** tras pruebas exhaustivas

2. **La arquitectura del modelo importa m√°s que el tama√±o**:
   - FLAN-T5 Base > GPT-2 Large para esta tarea
   - **Los modelos causales no funcionan bien sin fine-tuning espec√≠fico**

3. **APIs gratuitas tienen limitaciones severas**:
   - Migraci√≥n a modelos locales fue necesaria
   - Trade-off: RAM vs estabilidad (vale la pena)

4. **El chunking es cr√≠tico**:
   - Balance entre contexto y precisi√≥n
   - Overlap evita p√©rdida de informaci√≥n

5. **Pinecone simplifica vector search**:
   - Pero tiene limitaciones en plan gratuito
   - Alternativas: FAISS, Chroma (locales)

6. **Iteraci√≥n y prueba son fundamentales**:
   - Probar m√∫ltiples modelos fue esencial
   - Eliminar opciones no viables mejora la experiencia del usuario

### üéØ Recomendaci√≥n Final

Para un sistema RAG de producci√≥n sobre documentos en espa√±ol:

1. **Embeddings**: `multilingual-e5-large` (mejora sobre MiniLM)
2. **LLM**: FLAN-T5 Large o XL (si hay recursos)
3. **Vector Store**: Pinecone (escalable) o Qdrant (autohosted)
4. **Chunking**: 500-1000 caracteres con 10% overlap
5. **Infraestructura**: GPU para generaci√≥n r√°pida

---

## Referencias

- [LangChain Documentation](https://python.langchain.com/)
- [Pinecone Docs](https://docs.pinecone.io/)
- [Sentence-Transformers](https://www.sbert.net/)
- [FLAN-T5 Paper](https://arxiv.org/abs/2210.11416)
- [RAG Paper (Lewis et al.)](https://arxiv.org/abs/2005.11401)

---

**Autor**: Ezequiel Caama√±o